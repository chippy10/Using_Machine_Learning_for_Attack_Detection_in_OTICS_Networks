# -*- coding: utf-8 -*-
"""ICS Flow Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DjFefysbmnLT_-n_FCZt-6jiyTo_K5CB
"""

# Drive integration for Google Colab
from google.colab import drive
drive.mount('/content/drive')

# Data manipulation and analysis libraries
import pandas as pd
import numpy as np

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning libraries - model selection, preprocessing, and classification
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, fbeta_score, confusion_matrix, ConfusionMatrixDisplay


# Logistic Regression for classification
from sklearn.linear_model import LogisticRegression


# XGBoost for gradient boosting models
import xgboost as xgb


# Deep learning library for building neural networks
!pip install keras-tuner
!pip install scikeras
!pip install keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from scikeras.wrappers import KerasClassifier




# Additional utility libraries
import zipfile
import math
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

import zipfile

# Extracting our Kaggle ICS Flow Dataset
with zipfile.ZipFile('/content/drive/My Drive/ics flow/ics_flow.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/ICS_Flow_Dataset')

# Loading the ICS Flow dataset from our google drive
ics_df = pd.read_csv('/content/ICS_Flow_Dataset/Dataset.csv')


# Display all the columns in the dataset
ics_df.columns

# Adjust Pandas settings to display all columns
pd.set_option('display.max_columns', None)

# Get descriptive statistics for all columns
ics_df.describe()

# Display the first 5 rows of the dataset
ics_df.head()

# Checking for missing values in each column
missing_values = ics_df.isnull().sum()


# Display columns with missing values
missing_values[missing_values > 0]

# Plotting histograms for selected features
important_columns = ['sPackets', 'rPackets', 'duration', 'sBytesSum', 'rBytesSum']
ics_df[important_columns].hist(bins=30, figsize=(12, 8))
plt.show()

# Plot the relationship between duration and packet count
sns.scatterplot(x='duration', y='sPackets', data=ics_df)
plt.show()

# Check unique classes for IT system
print("Unique classes for IT_M_Label:")
print(ics_df['IT_M_Label'].unique())

# Check unique classes for Network Security Technology system
print("Unique classes for NST_M_Label:")
print(ics_df['NST_M_Label'].unique())


# Class distribution for IT system
print("Class distribution for IT_M_Label:")
print(ics_df['IT_M_Label'].value_counts())

# Class distribution for NST system
print("Class distribution for NST_M_Label:")
print(ics_df['NST_M_Label'].value_counts())

# Check for missing values in the dataset
print("Missing values in each column:")
print(ics_df.isnull().sum())

# Calculate the percentage of missing values in each column
missing_percentage = (ics_df.isnull().sum() / len(ics_df)) * 100

# Display columns with missing values and their percentages
missing_info = pd.DataFrame({
    'Missing Values': ics_df.isnull().sum(),
    'Percentage': missing_percentage
})

# Filter to show only columns with missing values
missing_info = missing_info[missing_info['Missing Values'] > 0]

print(missing_info)

# Display only the columns that have missing values
missing_columns = ics_df.columns[ics_df.isnull().any()]
print("Columns with missing data:", missing_columns)

# Display rows with any missing values
print("Sample rows with missing values:")
print(ics_df[ics_df.isnull().any(axis=1)].head(10))  # Display the first 10 rows with missing values

# Display rows that have missing values
rows_with_missing = ics_df[ics_df.isnull().any(axis=1)]
print(rows_with_missing)

# Filter rows to include all attack types and "Normal" in IT_M_Label or NST_M_Label
filtered_df = ics_df[(ics_df['IT_M_Label'].isin(['Normal', 'mitm', 'ddos', 'replay', 'port-scan', 'ip-scan'])) |
                     (ics_df['NST_M_Label'].isin(['Normal', 'mitm', 'ddos', 'replay', 'port-scan', 'ip-scan']))]

# Check the shape of the filtered dataset
print(f"Shape of dataset after including all attack types and Normal: {filtered_df.shape}")

# Optionally, check the first few rows to ensure it's correct
print(filtered_df.head())

# Keep rows where IT_M_Label or NST_M_Label includes any attack type or 'Normal'
filtered_df = ics_df[(ics_df['IT_M_Label'].isin(['mitm', 'ddos', 'Normal', 'replay', 'port-scan', 'ip-scan'])) |
                     (ics_df['NST_M_Label'].isin(['mitm', 'ddos', 'Normal', 'replay', 'port-scan', 'ip-scan']))]

# Verify the remaining unique labels
print("Unique values in IT_M_Label after filtering:", filtered_df['IT_M_Label'].unique())
print("Unique values in NST_M_Label after filtering:", filtered_df['NST_M_Label'].unique())

# Check the shape of the dataset after filtering for all attacks and Normal
print(f"Shape of dataset after including all attack types and Normal: {filtered_df.shape}")

# Drop rows with missing values
cleaned_filtered_df = filtered_df.dropna()

# Check the new shape
print(f"Shape of dataset after dropping rows with missing values: {cleaned_filtered_df.shape}")

# Filter rows to include all attack types and "Normal" in IT_M_Label or NST_M_Label
filtered_df = ics_df[(ics_df['IT_M_Label'].isin(['Normal', 'mitm', 'ddos', 'replay', 'port-scan', 'ip-scan'])) |
                     (ics_df['NST_M_Label'].isin(['Normal', 'mitm', 'ddos', 'replay', 'port-scan', 'ip-scan']))]

# Check the shape of the filtered dataset
print(f"Shape of dataset after including all attack types and Normal: {filtered_df.shape}")

# Check the first few rows to ensure it's correct
print(filtered_df.head())

# Drop rows with missing values
cleaned_filtered_df = filtered_df.dropna()

# Check the new shape
print(f"Shape of dataset after dropping rows with missing values: {cleaned_filtered_df.shape}")

# ********* Random Forest Algorithm *********

# Prepare features and use unencoded target directly
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']
ics_rf = ics_df.drop(columns=columns_to_drop)  # Features DataFrame
ics_rf_target = ics_df['NST_M_Label']  # Unencoded target as before

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(ics_rf, ics_rf_target, test_size=0.3, random_state=42)

# Train Random Forest without hyperparameter tuning
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred = rf_classifier.predict(X_test)

# Evaluate the model without hyperparameter tuning
print("Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred))
print("Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred))

# Overall metrics before hyperparameter tuning
overall_accuracy = accuracy_score(y_test, y_pred)
overall_precision = precision_score(y_test, y_pred, average='weighted')
overall_recall = recall_score(y_test, y_pred, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')

print("\nOverall Metrics (Without Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy:.4f}")
print(f"Precision (Weighted): {overall_precision:.4f}")
print(f"Recall (Weighted): {overall_recall:.4f}")
print(f"F2 Score (Weighted): {overall_f2:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting feature importances histogram before hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(ics_rf.columns, rf_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importances (Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning with RandomizedSearchCV for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

random_search_rf = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fitting the randomized search model
random_search_rf.fit(X_train, y_train)

# Best estimator after random search
best_rf_classifier = random_search_rf.best_estimator_
y_pred_tuned = best_rf_classifier.predict(X_test)

# Evaluate the model after hyperparameter tuning
print("\nClassification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_tuned))
print("Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_tuned, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_tuned:.4f}")
print(f"Precision (Weighted): {overall_precision_tuned:.4f}")
print(f"Recall (Weighted): {overall_recall_tuned:.4f}")
print(f"F2 Score (Weighted): {overall_f2_tuned:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting feature importances histogram after hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(ics_rf.columns, best_rf_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importances (After Hyperparameter Tuning)")
plt.show()

# ********* XGBoost Algorithm *********

from sklearn.preprocessing import LabelEncoder

# Prepare features
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']
ics_xgb = ics_df.drop(columns=columns_to_drop)

# Encode the target
label_encoder = LabelEncoder()
ics_df['NST_M_Label_encoded'] = label_encoder.fit_transform(ics_df['NST_M_Label'])
ics_xgb_target = ics_df['NST_M_Label_encoded']

# Split the data into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(ics_xgb, ics_xgb_target, test_size=0.3, random_state=42)

# Train XGBoost model without hyperparameter tuning
xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)
xgb_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_encoded = xgb_classifier.predict(X_test)

# Decode predictions and true labels back to original labels
y_pred = label_encoder.inverse_transform(y_pred_encoded)
y_test_original = label_encoder.inverse_transform(y_test)

# Evaluate the model without hyperparameter tuning
print("XGBoost - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test_original, y_pred))
print("XGBoost - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test_original, y_pred))

# Overall metrics before hyperparameter tuning
overall_accuracy = accuracy_score(y_test_original, y_pred)
overall_precision = precision_score(y_test_original, y_pred, average='weighted')
overall_recall = recall_score(y_test_original, y_pred, average='weighted')
overall_f2 = fbeta_score(y_test_original, y_pred, beta=2, average='weighted')

print("\nOverall Metrics (Without Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy:.4f}")
print(f"Precision (Weighted): {overall_precision:.4f}")
print(f"Recall (Weighted): {overall_recall:.4f}")
print(f"F2 Score (Weighted): {overall_f2:.4f}")
print(f"Support (Total Samples): {len(y_test_original)}")

# Feature Importance Plot before tuning
plt.figure(figsize=(15, 10))
plt.barh(ics_xgb.columns, xgb_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importances (XGBoost - Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning with RandomizedSearchCV for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

random_search = RandomizedSearchCV(
    estimator=xgb.XGBClassifier(eval_metric='mlogloss', random_state=42),
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fitting the randomized search model
random_search.fit(X_train, y_train)

# Best estimator after random search
best_xgb_classifier = random_search.best_estimator_
y_pred_tuned_encoded = best_xgb_classifier.predict(X_test)

# Decode the predictions for the tuned model
y_pred_tuned = label_encoder.inverse_transform(y_pred_tuned_encoded)

# Evaluate the model after hyperparameter tuning
print("\nXGBoost - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test_original, y_pred_tuned))
print("XGBoost - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test_original, y_pred_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test_original, y_pred_tuned)
overall_precision_tuned = precision_score(y_test_original, y_pred_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test_original, y_pred_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test_original, y_pred_tuned, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_tuned:.4f}")
print(f"Precision (Weighted): {overall_precision_tuned:.4f}")
print(f"Recall (Weighted): {overall_recall_tuned:.4f}")
print(f"F2 Score (Weighted): {overall_f2_tuned:.4f}")
print(f"Support (Total Samples): {len(y_test_original)}")

# Feature Importance Plot after tuning
plt.figure(figsize=(20, 10))
plt.barh(ics_xgb.columns, best_xgb_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importances (XGBoost - After Hyperparameter Tuning)")
plt.show()

# ********* KNN Algorithm *********

# Prepare Features
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']

ics_df.dropna(inplace=True)

ics_knn = ics_df.drop(columns=columns_to_drop)  # Features DataFrame
ics_knn_target = ics_df['NST_M_Label']  # Use unencoded `NST_M_Label` as the target

# Split the data into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(ics_knn, ics_knn_target, test_size=0.3, random_state=42)

# Train KNN model without hyperparameter tuning
knn_classifier = KNeighborsClassifier(n_neighbors=4)
knn_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn = knn_classifier.predict(X_test)

# Evaluate the model without hyperparameter tuning
print("KNN - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_knn))
print("KNN - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_knn))

# Overall metrics before hyperparameter tuning
overall_accuracy = accuracy_score(y_test, y_pred_knn)
overall_precision = precision_score(y_test, y_pred_knn, average='weighted')
overall_recall = recall_score(y_test, y_pred_knn, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_knn, beta=2, average='weighted')

print("\nOverall Metrics (Without Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy:.4f}")
print(f"Precision (Weighted): {overall_precision:.4f}")
print(f"Recall (Weighted): {overall_recall:.4f}")
print(f"F2 Score (Weighted): {overall_f2:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Determine layout for histogram dynamically
num_features = ics_knn.shape[1]
num_rows = math.ceil(math.sqrt(num_features))
num_cols = math.ceil(num_features / num_rows)

# Plotting feature histograms before hyperparameter tuning
ics_knn.hist(bins=20, layout=(num_rows, num_cols), figsize=(15, 10), edgecolor='black')
plt.suptitle("Feature Distributions (Before Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Hyperparameter tuning with RandomizedSearchCV for KNN
param_grid = {
    'n_neighbors': range(1, 20),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Using RandomizedSearchCV with a limited number of iterations
random_search_knn = RandomizedSearchCV(
    estimator=KNeighborsClassifier(),
    param_distributions=param_grid,
    n_iter=10,  # Number of parameter settings sampled
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fitting the random search model
random_search_knn.fit(X_train, y_train)

# Best estimator after random search
best_knn_classifier = random_search_knn.best_estimator_
y_pred_tuned = best_knn_classifier.predict(X_test)

# Evaluate the model after hyperparameter tuning
print("\nKNN - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_tuned))
print("KNN - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_tuned, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_tuned:.4f}")
print(f"Precision (Weighted): {overall_precision_tuned:.4f}")
print(f"Recall (Weighted): {overall_recall_tuned:.4f}")
print(f"F2 Score (Weighted): {overall_f2_tuned:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# ********* ANN Algorithm *********


# Prepare features
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']
ics_ann = ics_df.drop(columns=columns_to_drop)  # Features DataFrame

# Handle missing values
ics_ann = ics_ann.fillna(ics_ann.mean())

# Label encode the target for ANN
label_encoder = LabelEncoder()
ics_df['NST_M_Label_encoded'] = label_encoder.fit_transform(ics_df['NST_M_Label'])
ics_ann_target = ics_df['NST_M_Label_encoded']

# Split the data into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(ics_ann, ics_ann_target, test_size=0.3, random_state=42)

# Ensure no missing values in train and test sets
X_train = X_train.fillna(X_train.mean())
X_test = X_test.fillna(X_test.mean())

# Standardize the feature values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert the target to categorical (one-hot encoding)
y_train_categorical = to_categorical(y_train)
y_test_categorical = to_categorical(y_test)

# Step 1: Define the ANN model
def build_ann_model(dropout_rate=0.5):
    model = Sequential([
        Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(len(label_encoder.classes_), activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Step 2: Train ANN without hyperparameter tuning
print("Training ANN Without Hyperparameter Tuning...")
ann_model = build_ann_model(dropout_rate=0.5)
history_before = ann_model.fit(X_train_scaled, y_train_categorical, epochs=20, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model without hyperparameter tuning
y_pred_probs_before = ann_model.predict(X_test_scaled)
y_pred_before = y_pred_probs_before.argmax(axis=1)

# Classification report and accuracy score
print("\nANN - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_before))
print("ANN - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_before))

# Overall metrics before hyperparameter tuning
overall_accuracy_before = accuracy_score(y_test, y_pred_before)
overall_precision_before = precision_score(y_test, y_pred_before, average='weighted')
overall_recall_before = recall_score(y_test, y_pred_before, average='weighted')
overall_f2_before = fbeta_score(y_test, y_pred_before, beta=2, average='weighted')

print("\nOverall Metrics (Before Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_before:.4f}")
print(f"Precision (Weighted): {overall_precision_before:.4f}")
print(f"Recall (Weighted): {overall_recall_before:.4f}")
print(f"F2 Score (Weighted): {overall_f2_before:.4f}")

# Plot feature distributions before tuning
num_features = X_train.shape[1]
num_cols = 4
num_rows = math.ceil(num_features / num_cols)
X_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_scaled_df.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (Before Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Step 3: Hyperparameter tuning
print("\nTraining ANN With Hyperparameter Tuning...")
ann_classifier = KerasClassifier(build_fn=build_ann_model, verbose=1)  # Dropout rate is fixed in build_fn

# Define hyperparameter grid (exclude dropout_rate here)
param_grid = {
    'epochs': [20, 30],
    'batch_size': [16, 32]
}

# RandomizedSearchCV for hyperparameter tuning
random_search_ann = RandomizedSearchCV(
    estimator=ann_classifier,
    param_distributions=param_grid,
    n_iter=5,
    cv=3,
    scoring='accuracy',
    random_state=42
)

# Fit the model using randomized search
random_search_ann.fit(X_train_scaled, y_train_categorical)

# Step 4: Evaluate the model after hyperparameter tuning
y_pred_probs_after = random_search_ann.best_estimator_.predict(X_test_scaled)
y_pred_after = y_pred_probs_after.argmax(axis=1)

# Classification report and accuracy score after tuning
print("\nANN - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_after))
print("ANN - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_after))

# Overall metrics after hyperparameter tuning
overall_accuracy_after = accuracy_score(y_test, y_pred_after)
overall_precision_after = precision_score(y_test, y_pred_after, average='weighted')
overall_recall_after = recall_score(y_test, y_pred_after, average='weighted')
overall_f2_after = fbeta_score(y_test, y_pred_after, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_after:.4f}")
print(f"Precision (Weighted): {overall_precision_after:.4f}")
print(f"Recall (Weighted): {overall_recall_after:.4f}")
print(f"F2 Score (Weighted): {overall_f2_after:.4f}")

# Plot feature distributions after tuning
X_scaled_df.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (After Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# ********* SVM Algorithm *********

# Prepare features
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']
ics_svm = ics_df.drop(columns=columns_to_drop)  # Features DataFrame

# Handle any missing values in features by filling with the mean of each column
ics_svm = ics_svm.fillna(ics_svm.mean())
ics_svm_target = ics_df['NST_M_Label']  # Target

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(ics_svm, ics_svm_target, test_size=0.3, random_state=42)

# Train SVM model without hyperparameter tuning
svm_classifier = SVC(random_state=42)
svm_classifier.fit(X_train, y_train)
y_pred_svm = svm_classifier.predict(X_test)

# Evaluate the model without hyperparameter tuning, suppressing UndefinedMetricWarning
print("SVM - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_svm, zero_division=0))  # zero_division=0 handles undefined metrics
print("SVM - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_svm))

# Overall metrics before hyperparameter tuning, with zero_division in precision_score
overall_accuracy = accuracy_score(y_test, y_pred_svm)
overall_precision = precision_score(y_test, y_pred_svm, average='weighted', zero_division=0)
overall_recall = recall_score(y_test, y_pred_svm, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_svm, beta=2, average='weighted', zero_division=0)

print("\nOverall Metrics (Without Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy:.4f}")
print(f"Precision (Weighted): {overall_precision:.4f}")
print(f"Recall (Weighted): {overall_recall:.4f}")
print(f"F2 Score (Weighted): {overall_f2:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting Confusion Matrix for SVM without hyperparameter tuning
conf_matrix_initial = confusion_matrix(y_test, y_pred_svm)
ConfusionMatrixDisplay(conf_matrix_initial, display_labels=svm_classifier.classes_).plot(cmap="Blues")
plt.title("SVM Confusion Matrix (Without Hyperparameter Tuning)")
plt.show()


# Hyperparameter tuning with optimized RandomizedSearchCV for SVM
param_grid = {
    'C': [0.1, 1],  # Reduced range for faster tuning
    'kernel': ['linear', 'rbf'],  # Fewer kernel options for faster results
    'gamma': ['scale']  # Single value for gamma to simplify search
}

random_search_svm = RandomizedSearchCV(
    estimator=SVC(random_state=42),
    param_distributions=param_grid,
    n_iter=5,  # Lowered the number of iterations
    cv=3,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fitting the random search model
random_search_svm.fit(X_test, y_test)

# Best estimator after random search
best_svm_classifier = random_search_svm.best_estimator_
y_pred_svm_tuned = best_svm_classifier.predict(X_test)

# Evaluate the model after hyperparameter tuning
print("\nSVM - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_svm_tuned))
print("SVM - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_svm_tuned, zero_division=0))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_svm_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_svm_tuned, average='weighted', zero_division=0)
overall_recall_tuned = recall_score(y_test, y_pred_svm_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_svm_tuned, beta=2, average='weighted', zero_division=0)

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_tuned:.4f}")
print(f"Precision (Weighted): {overall_precision_tuned:.4f}")
print(f"Recall (Weighted): {overall_recall_tuned:.4f}")
print(f"F2 Score (Weighted): {overall_f2_tuned:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_svm_tuned)
ConfusionMatrixDisplay(conf_matrix, display_labels=best_svm_classifier.classes_).plot(cmap="Blues")
plt.title("SVM Confusion Matrix (With Hyperparameter Tuning)")
plt.show()

# ********* J48 Algorithm (Decision Tree Classifier) *********



# Prepare features
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']
ics_j48 = ics_df.drop(columns=columns_to_drop)  # Features DataFrame
ics_j48_target = ics_df['NST_M_Label']  # Unencoded target as before

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(ics_j48, ics_j48_target, test_size=0.3, random_state=42)

# Train Decision Tree (J48) without hyperparameter tuning
j48_classifier = DecisionTreeClassifier(random_state=42)
j48_classifier.fit(X_train, y_train)
y_pred = j48_classifier.predict(X_test)

# Evaluate the model without hyperparameter tuning
print("Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred))
print("Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred))

# Overall metrics before hyperparameter tuning
overall_accuracy = accuracy_score(y_test, y_pred)
overall_precision = precision_score(y_test, y_pred, average='weighted')
overall_recall = recall_score(y_test, y_pred, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')

print("\nOverall Metrics (Without Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy:.4f}")
print(f"Precision (Weighted): {overall_precision:.4f}")
print(f"Recall (Weighted): {overall_recall:.4f}")
print(f"F2 Score (Weighted): {overall_f2:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting feature importances histogram before hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(ics_j48.columns, j48_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importances (Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning with RandomizedSearchCV for Decision Tree
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']  # J48 uses "gini" (similar to C4.5)
}

random_search_j48 = RandomizedSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fitting the randomized search model
random_search_j48.fit(X_train, y_train)

# Best estimator after random search
best_j48_classifier = random_search_j48.best_estimator_
y_pred_tuned = best_j48_classifier.predict(X_test)

# Evaluate the model after hyperparameter tuning
print("\nClassification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_tuned))
print("Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_tuned, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_tuned:.4f}")
print(f"Precision (Weighted): {overall_precision_tuned:.4f}")
print(f"Recall (Weighted): {overall_recall_tuned:.4f}")
print(f"F2 Score (Weighted): {overall_f2_tuned:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting feature importances histogram after hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(ics_j48.columns, best_j48_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importances (After Hyperparameter Tuning)")
plt.show()

# ********* Logistic Regression Algorithm *********


# Prepare features
columns_to_drop = ['NST_M_Label', 'IT_M_Label', 'sAddress', 'rAddress', 'sMACs', 'rMACs',
                   'sIPs', 'rIPs', 'startDate', 'endDate', 'protocol', 'start', 'end',
                   'startOffset', 'endOffset']
ics_lr = ics_df.drop(columns=columns_to_drop)  # Features DataFrame
ics_lr_target = ics_df['NST_M_Label']  # Unencoded target as before

# Drop rows with missing values in the features or target columns
ics_lr = ics_lr.dropna()
ics_lr_target = ics_lr_target[ics_lr.index]

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(ics_lr, ics_lr_target, test_size=0.3, random_state=42)

# Train Logistic Regression without hyperparameter tuning
lr_classifier = LogisticRegression(max_iter=1000, random_state=42)  # Increase max_iter if needed for convergence
lr_classifier.fit(X_train, y_train)
y_pred = lr_classifier.predict(X_test)

# Evaluate the model without hyperparameter tuning
print("Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred))
print("Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred))

# Overall metrics before hyperparameter tuning
overall_accuracy = accuracy_score(y_test, y_pred)
overall_precision = precision_score(y_test, y_pred, average='weighted')
overall_recall = recall_score(y_test, y_pred, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')

print("\nOverall Metrics (Without Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy:.4f}")
print(f"Precision (Weighted): {overall_precision:.4f}")
print(f"Recall (Weighted): {overall_recall:.4f}")
print(f"F2 Score (Weighted): {overall_f2:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Feature importance in Logistic Regression can be evaluated using the coefficients
plt.figure(figsize=(15, 10))
plt.barh(ics_lr.columns, np.abs(lr_classifier.coef_[0]))  # Take absolute values of coefficients
plt.xlabel("Feature Importance (Coefficient Magnitude)")
plt.title("Feature Importance (Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning with RandomizedSearchCV for Logistic Regression
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Different regularization methods
    'C': np.logspace(-4, 4, 20),                    # Inverse of regularization strength
    'solver': ['saga', 'liblinear'],                # Solvers that support L1 and L2 regularization
}

random_search_lr = RandomizedSearchCV(
    estimator=LogisticRegression(max_iter=1000, random_state=42),
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fitting the randomized search model
random_search_lr.fit(X_train, y_train)

# Best estimator after random search
best_lr_classifier = random_search_lr.best_estimator_
y_pred_tuned = best_lr_classifier.predict(X_test)

# Evaluate the model after hyperparameter tuning
print("\nClassification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_tuned))
print("Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_tuned, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_tuned:.4f}")
print(f"Precision (Weighted): {overall_precision_tuned:.4f}")
print(f"Recall (Weighted): {overall_recall_tuned:.4f}")
print(f"F2 Score (Weighted): {overall_f2_tuned:.4f}")
print(f"Support (Total Samples): {len(y_test)}")

# Plotting feature importance after hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(ics_lr.columns, np.abs(best_lr_classifier.coef_[0]))
plt.xlabel("Feature Importance (Coefficient Magnitude)")
plt.title("Feature Importance (After Hyperparameter Tuning)")
plt.show()

# prompt: Can you take all the algorithms(RD,XGBoost,KNN,ANN,SVM,J48,LR )results "Accuracy Score (With Hyperparameter Tuning),Precision , Recall and F2 Score
# as percentage%"  after hyperparameter tuning and make a matrix out of them and every algorthim has own color

import pandas as pd
import matplotlib.pyplot as plt

# Sample data (replace with your actual results)
data = {
    'Algorithm': ['RD', 'XGBoost', 'KNN', 'ANN', 'SVM', 'J48', 'LR'],
    'Accuracy': [98, 98, 97, 99, 81, 98, 99],
    'Precision': [90, 94, 85, 88, 87, 90, 92],
    'Recall': [91, 96, 86, 89, 88, 90, 94],
    'F2_Score': [90, 95, 87, 89, 88, 90, 93]
}
df = pd.DataFrame(data)

# Define colors for each algorithm
colors = ['skyblue', 'orange', 'green', 'red', 'purple', 'brown', 'pink']

# Create the matrix plot
plt.figure(figsize=(10, 6))
for i, metric in enumerate(df.columns[1:]):  # Iterate through metrics
  plt.bar(df['Algorithm'], df[metric], label=metric, color=colors)

plt.title('Algorithm Performance Matrix')
plt.xlabel('Algorithm')
plt.ylabel('Percentage (%)')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data preparation
algorithms = ['RD', 'XGBoost', 'KNN', 'ANN', 'SVM', 'J48', 'LR']
metrics = ['Accuracy', 'Recall', 'F1-score', 'Precision']

# Replace the following with your actual data
data = {
    'RD': [98.93, 99.33, 98.94, 98.96],
    'XGBoost': [98.88, 99.27, 98.88, 98.91],
    'KNN': [97.29, 97.21, 97.29, 97.23],
    'ANN': [99.99, 99.99, 99.99, 99.99],
    'SVM': [81.06, 91.6, 92.2, 90.5],
    'J48': [98.90, 88.2, 88.9, 87.4],
    'LR': [99.96, 86.0, 86.7, 85.2]
}

# Create a grouped bar chart
x = np.arange(len(algorithms))
width = 0.2

fig, ax = plt.subplots(figsize=(12, 6))

for i, metric in enumerate(metrics):
    values = [data[alg][i] for alg in algorithms]
    ax.bar(x + i * width, values, width, label=metric)

# Customization
ax.set_xlabel('Algorithms')
ax.set_ylabel('Detection Rate (%)')
ax.set_title('Comparison of Detection Rates by Algorithm')
ax.set_xticks(x + width * 1.5)
ax.set_xticklabels(algorithms)
ax.legend(title='Metrics')

# Show plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data preparation
algorithms = ['RD', 'XGBoost', 'KNN', 'ANN', 'SVM', 'J48', 'LR']
metrics = ['Accuracy', 'Recall', 'F1-score', 'Precision']

# Replace these values with your actual data
data = {
    'RD': [98.93, 99.33, 98.94, 98.96],
    'XGBoost': [98.88, 99.27, 98.88, 98.91],
    'KNN': [97.29, 97.21, 97.29, 97.23],
    'ANN': [99.99, 99.99, 99.99, 99.99],
    'SVM': [81.06, 91.6, 92.2, 90.5],
    'J48': [98.90, 88.2, 88.9, 87.4],
    'LR': [99.96, 86.0, 86.7, 85.2]
}

# Create a NumPy array for each metric
accuracy = [data[alg][0] for alg in algorithms]
recall = [data[alg][1] for alg in algorithms]
f1_score = [data[alg][2] for alg in algorithms]
precision = [data[alg][3] for alg in algorithms]

# Bar width and positions
x = np.arange(len(algorithms))
width = 0.2

# Create the plot
fig, ax = plt.subplots(figsize=(14, 7))

# Add bars for each metric
bars1 = ax.bar(x - 1.5 * width, accuracy, width, label='Accuracy', color='#4CAF50')
bars2 = ax.bar(x - 0.5 * width, recall, width, label='Recall', color='#2196F3')
bars3 = ax.bar(x + 0.5 * width, f1_score, width, label='F1-score', color='#FFC107')
bars4 = ax.bar(x + 1.5 * width, precision, width, label='Precision', color='#FF5722')

# Add value labels on top of the bars
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.1f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # Offset for text
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9)

add_labels(bars1)
add_labels(bars2)
add_labels(bars3)
add_labels(bars4)

# Customization
ax.set_xlabel('Algorithms', fontsize=12)
ax.set_ylabel('Detection Rate (%)', fontsize=12)
ax.set_title('Detection Rate Comparison of Algorithms', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(algorithms, fontsize=11)
ax.legend(title='Metrics', fontsize=11)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

# prompt: # prompt: i wanna plot  for Normal ,ddos,ip-scan,mitm,port-scan,replay  4 type of plot

import matplotlib.pyplot as plt

# Sample traffic data (replace with your actual data)
# Data for different attack types
normal_traffic = [100, 105, 110, 108, 115]
ddos_traffic = [150, 160, 170, 165, 180]
ip_scan_traffic = [80, 85, 90, 82, 95]
mitm_traffic = [200, 210, 220, 215, 230]
port_scan_traffic = [120, 125, 130, 128, 135]
replay_traffic = [90, 95, 100, 98, 105]

# Time points (replace with your actual timestamps)
timestamps = range(1, 6)


# Plotting
plt.figure(figsize=(10, 6))

plt.plot(timestamps, normal_traffic, label='Normal', marker='o', linestyle='-')
plt.plot(timestamps, ddos_traffic, label='DDoS', marker='o', linestyle='--')
plt.plot(timestamps, ip_scan_traffic, label='IP-Scan', marker='o', linestyle='-.')
plt.plot(timestamps, mitm_traffic, label='MITM', marker='o', linestyle=':')
plt.plot(timestamps, port_scan_traffic, label='Port-Scan', marker='o', linestyle='-')
plt.plot(timestamps, replay_traffic, label='Replay', marker='o', linestyle='--')

plt.xlabel('Timestamp')
plt.ylabel('Traffic Value')
plt.title('Traffic Pattern by Attack Type')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# prompt: # prompt: i wanna draw line for Normal ,ddos,ip-scan,mitm,port-scan,replay and each of them  have own line

import matplotlib.pyplot as plt

# Sample traffic data (replace with your actual data)
# Using lists for demonstration, but a Pandas DataFrame would be better for real data
timestamps = list(range(1, 11))  # Example timestamps
normal_traffic = [10, 12, 15, 14, 16, 18, 20, 19, 22, 25]
ddos_traffic = [50, 60, 70, 65, 80, 90, 100, 95, 110, 120]  # Example DDoS traffic
ip_scan_traffic = [11,13, 16, 15, 17, 19, 21, 20, 23, 26]
mitm_traffic = [8,10, 13, 12, 14, 16, 18, 17, 20, 23]
port_scan_traffic = [12,14, 17, 16, 18, 20, 22, 21, 24, 27]
replay_traffic = [7, 9, 12, 11, 13, 15, 17, 16, 19, 22]


# Create the plot
plt.figure(figsize=(12, 6))

# Plot each attack type with a different line style
plt.plot(timestamps, normal_traffic, label='Normal', linestyle='-', color='blue')
plt.plot(timestamps, ddos_traffic, label='DDoS', linestyle='--', color='red')
plt.plot(timestamps, ip_scan_traffic, label='IP-Scan', linestyle='-.', color='green')
plt.plot(timestamps, mitm_traffic, label='MITM', linestyle=':', color='purple')
plt.plot(timestamps, port_scan_traffic, label='Port-Scan', linestyle='-', color='orange')
plt.plot(timestamps, replay_traffic, label='Replay', linestyle='--', color='black')


plt.xlabel('Timestamp')
plt.ylabel('Traffic Value')
plt.title('Traffic Pattern by Attack Type')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()