# -*- coding: utf-8 -*-
"""Copy of Power System Dataset v2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FSgxzTwCDsCqL-Tur31N0HBRycN63U-d

Problem Definition for Power System Dataset:


1. Context of the Problem
The Power System dataset represents network traffic in a power system infrastructure, which is part of critical infrastructure similar to industrial control systems (ICS). Power systems are highly vulnerable to cyberattacks, including MITM and DDoS attacks. A compromised power system can lead to widespread disruptions, including outages and failures in energy transmission.

Given the importance of these systems to national security and daily life, detecting cyber threats in network traffic is crucial to ensure the reliable and safe operation of power grids and utilities.



2. Objective of the Project
The objective of this project is to develop a machine learning-based anomaly detection model for the Power System dataset that can accurately detect MITM and DDoS attacks. The model should be able to classify the network traffic into normal and attack-based categories, with a particular focus on ensuring high detection accuracy for these specific cyberattack types.



3. Key Questions
The key questions driving this project are:


- How can we distinguish between normal network traffic and attack traffic in the Power System dataset?
- What are the key features that help detect MITM and DDoS attacks in this dataset?
- Which machine learning model provides the best performance for detecting attacks within this critical infrastructure?
- How can we handle imbalanced data in detecting these rare but dangerous attacks?
- How effective are feature engineering techniques (e.g., creating new features, transforming data) in improving model performance?



 4. Problem Formulation
This is a supervised classification problem where the goal is to classify network flows as either normal or under attack (MITM, DDoS, etc.) based on the data available in the Power System dataset. The challenge here is:

- Handling the imbalanced nature of the data (e.g., the scarcity of attack instances).
- Using key features that provide enough information to distinguish normal behavior from attack traffic.
- Achieving high accuracy in detecting rare attacks like MITM and DDoS.
- To achieve this, we will preprocess the data, engineer features, and evaluate various machine learning models.
"""

from google.colab import drive
import zipfile

# Mount Google Drive
drive.mount('/content/drive')

# Unzipping the Power System Intrusion Dataset
with zipfile.ZipFile('/content/drive/My Drive/Power System.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/Power_System_Dataset')

# Drive integration for Google Colab
from google.colab import drive

# Data manipulation and analysis libraries
import pandas as pd
import numpy as np
!pip install keras-tuner
!pip install scikeras
!pip install keras

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning libraries - model selection, preprocessing, and classification
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, fbeta_score
from sklearn.tree import DecisionTreeClassifier
from scikeras.wrappers import KerasClassifier





# XGBoost library
import xgboost as xgb

# Handling class imbalance
from imblearn.over_sampling import SMOTE

# Deep learning library for neural networks
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.utils import to_categorical

# Additional utility libraries
import warnings
import math
import zipfile
from collections import Counter

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd

# We load the dataset into a DataFrame
df = pd.read_csv('/content/Power_System_Dataset/Train.csv')


# Display all the columns in the dataset
df.columns

# Adjust Pandas settings to display all columns
pd.set_option('display.max_columns', None)

# Get descriptive statistics for all columns
df.describe()

# We change the settings to display all columns
pd.set_option('display.max_columns', None)

# Now let's look at the first few rows again to see all columns
df.head()

# As we can observe, the Power System Dataset doesn't have missing values.

# Checking for missing values in each column
missing_values = df.isnull().sum()


# Display columns with missing values
missing_values[missing_values > 0]

# Importing numpy
import numpy as np

# Checking the non-numeric columns
non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns

# Display non-numeric columns
non_numeric_columns

# Display the list of all column names in the DataFrame
print(df.columns)

"""Now let's check the Histogram graph but before that, let's determine which are the key fields to use for checking this.

Step 1: Creating Histograms
Here are some key fields from the Power System Dataset that we will start with:

- threePhaseVSum: Sum of the voltages across the three phases.
- threePhaseCSum: Sum of the currents across the three phases.
- MU1VoltageAngleA, MU2VoltageAngleA: Voltage angles for different phases.
- threePhaseCurrentMU1, threePhaseCurrentMU2: Current for measurement units.
- duration: Time span of each traffic flow (if present in this dataset).


These fields are important for monitoring the system's health and identifying abnormal patterns. We will create histograms for these fields.


Step 2: Checking Relationships Between Two Variables
After plotting the histograms, weâ€™ll explore the relationship between two variables that might provide insights, such as:

- threePhaseVSum vs. threePhaseCSum: This can help understand how voltage and current behave together.
- MU1VoltageAngleA vs. threePhaseCurrentMU1: This relationship can show how voltage angles impact the current.
"""

# Important columns for the histograms
important_columns = ['threePhaseVSum', 'threePhaseCSum', 'MU1VoltageAngleA',
                     'MU2VoltageAngleA', 'threePhaseCurrentMU1', 'threePhaseCMU2']

# Plotting histograms for selected features
df[important_columns].hist(bins=30, figsize=(15, 10))
plt.suptitle('Histograms for Selected Features')
plt.show()

# Scatter plot to explore relationships between two variables (using columns that exist)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='threePhaseVSum', y='threePhaseCSum', data=df)
plt.title('Relationship between ThreePhaseVSum and ThreePhaseCSum')
plt.show()

# We check the structure of the dataset
df.info()

# Step 1: Check class distribution
print("Class distribution in the dataset:")
class_distribution = df['class'].value_counts()
print(class_distribution)

# Step 2: Check for missing values
print("\nMissing values in each column:")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

# Step 3: Separating features and target
# Assuming 'class' is the target column and the rest are features
X = df.drop(columns=['class'])  # Features
y = df['class']  # Target

# Display the first few rows of features and target
print("\nFirst few rows of the features (X):")
print(X.head())

print("\nFirst few rows of the target (y):")
print(y.head())

# Drop only rows related to 'Fault' attacks

# Dropping all rows where the 'class' is 'Fault'
df_filtered = df[df['class'] != 'Fault']

# Display the new shape of the dataset after filtering
print("New shape of the dataset after dropping 'Fault':", df_filtered.shape)

# Display the distinct attack types remaining in the 'class' column after filtering
print("Distinct attack types in 'class':", df_filtered['class'].unique())

# SMOTE Implementation (Before / After)

# Display class distribution before applying SMOTE
class_counts_before = Counter(df_filtered['class'])
classes, counts_before = zip(*class_counts_before.items())

# Apply SMOTE to balance the dataset
X = df_filtered.drop(columns=['class'])  # Features
y = df_filtered['class']  # Target

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)

# Display class distribution after applying SMOTE
class_counts_after = Counter(y_smote)
counts_after = [class_counts_after[cls] for cls in classes]  # Reorder for consistent class labels

# Plotting class distribution before and after SMOTE
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Before SMOTE
ax1.bar(classes, counts_before, color='skyblue')
ax1.set_title('Class Distribution Before SMOTE')
ax1.set_xlabel('Class')
ax1.set_ylabel('Count')

# After SMOTE
ax2.bar(classes, counts_after, color='salmon')
ax2.set_title('Class Distribution After SMOTE')
ax2.set_xlabel('Class')
ax2.set_ylabel('Count')

plt.tight_layout()
plt.show()

# Random Forest Implementation



# Copy the dataset
pwr_rf = df_filtered.copy()
X = pwr_rf.drop(columns=['class'])
y = pwr_rf['class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Initialize and train the Random Forest classifier without hyperparameter tuning
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred_rf = rf_classifier.predict(X_test)

# Evaluate without hyperparameter tuning
print("Random Forest - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_rf))
print("Random Forest - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_rf))

# Overall metrics
overall_accuracy = accuracy_score(y_test, y_pred_rf)
overall_precision = precision_score(y_test, y_pred_rf, average='weighted')
overall_recall = recall_score(y_test, y_pred_rf, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_rf, beta=2, average='weighted')
print(f"\nRandom Forest Overall Metrics (Without Hyperparameter Tuning): Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F2 Score: {overall_f2:.4f}")

# Plot feature importance before hyperparameter tuning
plt.figure(figsize=(10, 6))
plt.barh(X.columns, rf_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importance (Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning
param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}
grid_search_rf = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42), param_distributions=param_grid_rf, cv=3, scoring='accuracy', n_iter=5)
grid_search_rf.fit(X_train, y_train)
best_rf = grid_search_rf.best_estimator_
y_pred_rf_tuned = best_rf.predict(X_test)

# Evaluate with hyperparameter tuning
print("\nRandom Forest - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_rf_tuned))
print("Random Forest - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_rf_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_rf_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_rf_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_rf_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_rf_tuned, beta=2, average='weighted')
print(f"\nRandom Forest Overall Metrics (With Hyperparameter Tuning): Accuracy: {overall_accuracy_tuned:.4f}, Precision: {overall_precision_tuned:.4f}, Recall: {overall_recall_tuned:.4f}, F2 Score: {overall_f2_tuned:.4f}")

# Plot feature importance after hyperparameter tuning
plt.figure(figsize=(10, 6))
plt.barh(X.columns, best_rf.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importance (After Hyperparameter Tuning)")
plt.show()

#xgboost Implementation



# Copy the dataset
pwr_xgb = df_filtered.copy()
X = pwr_xgb.drop(columns=['class'])
y = pwr_xgb['class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Initialize and train the XGBoost classifier without hyperparameter tuning
xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)
xgb_classifier.fit(X_train, y_train)
y_pred_xgb = xgb_classifier.predict(X_test)

# Evaluate without hyperparameter tuning
print("XGBoost - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_xgb))
print("XGBoost - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_xgb))

# Overall metrics
overall_accuracy = accuracy_score(y_test, y_pred_xgb)
overall_precision = precision_score(y_test, y_pred_xgb, average='weighted')
overall_recall = recall_score(y_test, y_pred_xgb, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_xgb, beta=2, average='weighted')
print(f"\nXGBoost Overall Metrics (Without Hyperparameter Tuning): Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F2 Score: {overall_f2:.4f}")

# Plot feature importance before hyperparameter tuning
plt.figure(figsize=(10, 6))
plt.barh(X.columns, xgb_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importance (Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning
param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2]
}
grid_search_xgb = RandomizedSearchCV(
    estimator=xgb.XGBClassifier(eval_metric='mlogloss', random_state=42),
    param_distributions=param_grid_xgb,
    cv=3,
    scoring='accuracy',
    n_iter=5
)
grid_search_xgb.fit(X_train, y_train)
best_xgb = grid_search_xgb.best_estimator_
y_pred_xgb_tuned = best_xgb.predict(X_test)

# Evaluate with hyperparameter tuning
print("\nXGBoost - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_xgb_tuned))
print("XGBoost - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_xgb_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_xgb_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_xgb_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_xgb_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_xgb_tuned, beta=2, average='weighted')
print(f"\nXGBoost Overall Metrics (With Hyperparameter Tuning): Accuracy: {overall_accuracy_tuned:.4f}, Precision: {overall_precision_tuned:.4f}, Recall: {overall_recall_tuned:.4f}, F2 Score: {overall_f2_tuned:.4f}")

# Plot feature importance after hyperparameter tuning
plt.figure(figsize=(10, 6))
plt.barh(X.columns, best_xgb.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importance (After Hyperparameter Tuning)")
plt.show()

#  KNN Algorithm Implementation


# Copy the dataset
pwr_knn = df_filtered.copy()
X = pwr_knn.drop(columns=['class'])
y = pwr_knn['class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Initialize and train the KNN classifier without hyperparameter tuning
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train, y_train)
y_pred_knn = knn_classifier.predict(X_test)

# Evaluate without hyperparameter tuning
print("KNN - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_knn))
print("KNN - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_knn))

# Overall metrics without hyperparameter tuning
overall_accuracy = accuracy_score(y_test, y_pred_knn)
overall_precision = precision_score(y_test, y_pred_knn, average='weighted')
overall_recall = recall_score(y_test, y_pred_knn, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_knn, beta=2, average='weighted')
print(f"\nKNN Overall Metrics (Without Hyperparameter Tuning): Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F2 Score: {overall_f2:.4f}")

# Plot feature distributions before hyperparameter tuning
num_features = X.shape[1]
num_cols = 4
num_rows = math.ceil(num_features / num_cols)

plt.figure(figsize=(15, 10))
X.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (Before Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Hyperparameter tuning
param_grid_knn = {
    'n_neighbors': range(1, 20),
    'weights': ['uniform', 'distance']
}
grid_search_knn = RandomizedSearchCV(
    estimator=KNeighborsClassifier(),
    param_distributions=param_grid_knn,
    cv=3,
    scoring='accuracy',
    n_iter=5
)
grid_search_knn.fit(X_train, y_train)
best_knn = grid_search_knn.best_estimator_
y_pred_knn_tuned = best_knn.predict(X_test)

# Evaluate with hyperparameter tuning
print("\nKNN - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_knn_tuned))
print("KNN - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_knn_tuned))

# Overall metrics with hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_knn_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_knn_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_knn_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_knn_tuned, beta=2, average='weighted')
print(f"\nKNN Overall Metrics (With Hyperparameter Tuning): Accuracy: {overall_accuracy_tuned:.4f}, Precision: {overall_precision_tuned:.4f}, Recall: {overall_recall_tuned:.4f}, F2 Score: {overall_f2_tuned:.4f}")

# Plot feature distributions after hyperparameter tuning (optional, if needed)
plt.figure(figsize=(15, 10))
X.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (After Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# SVM Algorithm Implementation


# Copy the dataset
pwr_svm = df_filtered.copy()
X = pwr_svm.drop(columns=['class'])
y = pwr_svm['class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Determine layout for histograms dynamically
num_features = X.shape[1]
num_cols = 4
num_rows = math.ceil(num_features / num_cols)

# Plot feature distributions before scaling
plt.figure(figsize=(15, 10))
X.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (Before Standardization)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Standardize the feature values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the SVM classifier without hyperparameter tuning
svm_classifier = SVC(kernel='rbf', random_state=42)
svm_classifier.fit(X_train_scaled, y_train)
y_pred_svm = svm_classifier.predict(X_test_scaled)

# Evaluate without hyperparameter tuning
print("SVM - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_svm))
print("SVM - Accuracy Score:", accuracy_score(y_test, y_pred_svm))

# Overall metrics without hyperparameter tuning
overall_accuracy = accuracy_score(y_test, y_pred_svm)
overall_precision = precision_score(y_test, y_pred_svm, average='weighted')
overall_recall = recall_score(y_test, y_pred_svm, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_svm, beta=2, average='weighted')
print(f"\nSVM Overall Metrics (Without Hyperparameter Tuning): Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F2 Score: {overall_f2:.4f}")

# Hyperparameter tuning
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'poly', 'rbf'],
    'gamma': ['scale', 'auto']
}
grid_search_svm = RandomizedSearchCV(
    estimator=SVC(random_state=42),
    param_distributions=param_grid_svm,
    cv=3,
    scoring='accuracy',
    n_iter=5
)
grid_search_svm.fit(X_train_scaled, y_train)
best_svm = grid_search_svm.best_estimator_
y_pred_svm_tuned = best_svm.predict(X_test_scaled)

# Evaluate with hyperparameter tuning
print("\nSVM - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_svm_tuned))
print("SVM - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_svm_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_svm_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_svm_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_svm_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_svm_tuned, beta=2, average='weighted')
print(f"\nSVM Overall Metrics (With Hyperparameter Tuning): Accuracy: {overall_accuracy_tuned:.4f}, Precision: {overall_precision_tuned:.4f}, Recall: {overall_recall_tuned:.4f}, F2 Score: {overall_f2_tuned:.4f}")

# Plot feature distributions after scaling (optional)
X_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)
plt.figure(figsize=(15, 10))
X_scaled_df.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (After Standardization)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# LR Algorithm Implementation


# Copy the dataset
pwr_lr = df_filtered.copy()
X = pwr_lr.drop(columns=['class'])
y = pwr_lr['class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Determine layout for histograms dynamically
num_features = X.shape[1]
num_cols = 4
num_rows = math.ceil(num_features / num_cols)

# Plot feature distributions before scaling
plt.figure(figsize=(15, 10))
X.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (Before Standardization)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Standardize the feature values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the Logistic Regression classifier
lr_classifier = LogisticRegression(random_state=42, max_iter=100)
lr_classifier.fit(X_train_scaled, y_train)
y_pred_lr = lr_classifier.predict(X_test_scaled)

# Evaluate without hyperparameter tuning
print("LR - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_lr))
print("LR - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_lr))

# Overall metrics
overall_accuracy = accuracy_score(y_test, y_pred_lr)
overall_precision = precision_score(y_test, y_pred_lr, average='weighted')
overall_recall = recall_score(y_test, y_pred_lr, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_lr, beta=2, average='weighted')
print(f"\nLR Overall Metrics (Without Hyperparameter Tuning): Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F2 Score: {overall_f2:.4f}")

# Hyperparameter tuning for Logistic Regression
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear'],
    'penalty': ['l2']
}
grid_search_lr = RandomizedSearchCV(estimator=LogisticRegression(random_state=42, max_iter=100),
                                    param_distributions=param_grid_lr, cv=3, scoring='accuracy', n_iter=5)
grid_search_lr.fit(X_train_scaled, y_train)
best_lr = grid_search_lr.best_estimator_
y_pred_lr_tuned = best_lr.predict(X_test_scaled)

# Evaluate with hyperparameter tuning
print("\nLR - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_lr_tuned))
print("LR - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_lr_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_lr_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_lr_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_lr_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_lr_tuned, beta=2, average='weighted')
print(f"\nLR Overall Metrics (With Hyperparameter Tuning): Accuracy: {overall_accuracy_tuned:.4f}, Precision: {overall_precision_tuned:.4f}, Recall: {overall_recall_tuned:.4f}, F2 Score: {overall_f2_tuned:.4f}")

# Plot feature distributions after scaling
X_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)
plt.figure(figsize=(15, 10))
X_scaled_df.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (After Standardization)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# ANN Algorithm Implementation


# Copy the dataset
pwr_ann = df_filtered.copy()
X = pwr_ann.drop(columns=['class'])
y = pwr_ann['class']

# Encode the target labels to ensure compatibility with ANN
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Determine layout for histograms dynamically
num_features = X.shape[1]
num_cols = 4
num_rows = math.ceil(num_features / num_cols)

# Plot feature distributions before scaling
X.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (Before Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Standardize the feature values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert target to categorical (one-hot encoding for ANN)
y_train_categorical = to_categorical(y_train)
y_test_categorical = to_categorical(y_test)

# Step 1: Define the ANN model
def build_ann_model(dropout_rate=0.5):
    model = Sequential([
        Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(len(label_encoder.classes_), activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Step 2: Train ANN without hyperparameter tuning
ann_model = build_ann_model(dropout_rate=0.5)
history = ann_model.fit(X_train_scaled, y_train_categorical, epochs=20, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model without hyperparameter tuning
y_pred_probs_before = ann_model.predict(X_test_scaled)
y_pred_before = y_pred_probs_before.argmax(axis=1)

# Classification report and accuracy score
print("ANN - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_before))
print("ANN - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_before))

# Overall metrics before hyperparameter tuning
overall_accuracy_before = accuracy_score(y_test, y_pred_before)
overall_precision_before = precision_score(y_test, y_pred_before, average='weighted')
overall_recall_before = recall_score(y_test, y_pred_before, average='weighted')
overall_f2_before = fbeta_score(y_test, y_pred_before, beta=2, average='weighted')

print("\nOverall Metrics (Before Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_before:.4f}")
print(f"Precision (Weighted): {overall_precision_before:.4f}")
print(f"Recall (Weighted): {overall_recall_before:.4f}")
print(f"F2 Score (Weighted): {overall_f2_before:.4f}")


# Step 3: Hyperparameter tuning
ann_classifier = KerasClassifier(build_fn=build_ann_model, verbose=1, dropout_rate=0.5)

# Define hyperparameter grid
param_grid = {
    'epochs': [20, 30],
    'batch_size': [16, 32]
}

# RandomizedSearchCV for hyperparameter tuning
random_search_ann = RandomizedSearchCV(
    estimator=ann_classifier,
    param_distributions=param_grid,
    n_iter=5,
    cv=3,
    scoring='accuracy',
    random_state=42
)

# Fit the model using randomized search
random_search_ann.fit(X_train_scaled, y_train_categorical)

# Step 4: Predict using the best model after tuning
y_pred_probs_after = random_search_ann.best_estimator_.predict(X_test_scaled)
y_pred_after = y_pred_probs_after.argmax(axis=1)

# Classification report and accuracy score after tuning
print("\nANN - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_after))
print("ANN - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_after))

# Overall metrics after hyperparameter tuning
overall_accuracy_after = accuracy_score(y_test, y_pred_after)
overall_precision_after = precision_score(y_test, y_pred_after, average='weighted')
overall_recall_after = recall_score(y_test, y_pred_after, average='weighted')
overall_f2_after = fbeta_score(y_test, y_pred_after, beta=2, average='weighted')

print("\nOverall Metrics (With Hyperparameter Tuning):")
print(f"Accuracy: {overall_accuracy_after:.4f}")
print(f"Precision (Weighted): {overall_precision_after:.4f}")
print(f"Recall (Weighted): {overall_recall_after:.4f}")
print(f"F2 Score (Weighted): {overall_f2_after:.4f}")


# Plot feature distributions after scaling
X_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)
X_scaled_df.hist(bins=20, edgecolor='black', layout=(num_rows, num_cols), figsize=(15, 10))
plt.suptitle("Feature Distributions (After Hyperparameter Tuning)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

#  J48 (C4.5) Decision Tree Algorithm Implementation


# Copy the dataset
pwr_j48 = df_filtered.copy()
X = pwr_j48.drop(columns=['class'])
y = pwr_j48['class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Initialize and train the Decision Tree classifier without hyperparameter tuning
j48_classifier = DecisionTreeClassifier(random_state=42, criterion='entropy')  # 'entropy' is used here to mimic the C4.5 algorithm
j48_classifier.fit(X_train, y_train)
y_pred_j48 = j48_classifier.predict(X_test)

# Evaluate without hyperparameter tuning
print("J48 Decision Tree - Classification Report (Without Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_j48))
print("J48 Decision Tree - Accuracy Score (Without Hyperparameter Tuning):", accuracy_score(y_test, y_pred_j48))

# Overall metrics
overall_accuracy = accuracy_score(y_test, y_pred_j48)
overall_precision = precision_score(y_test, y_pred_j48, average='weighted')
overall_recall = recall_score(y_test, y_pred_j48, average='weighted')
overall_f2 = fbeta_score(y_test, y_pred_j48, beta=2, average='weighted')
print(f"\nJ48 Decision Tree Overall Metrics (Without Hyperparameter Tuning): Accuracy: {overall_accuracy:.4f}, Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F2 Score: {overall_f2:.4f}")

# Plot feature importance before hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(X.columns, j48_classifier.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importance (Before Hyperparameter Tuning)")
plt.show()

# Hyperparameter tuning for Decision Tree (J48)
param_grid_j48 = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
grid_search_j48 = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42, criterion='entropy'),
                                     param_distributions=param_grid_j48, cv=3, scoring='accuracy', n_iter=5)
grid_search_j48.fit(X_train, y_train)
best_j48 = grid_search_j48.best_estimator_
y_pred_j48_tuned = best_j48.predict(X_test)

# Evaluate with hyperparameter tuning
print("\nJ48 Decision Tree - Classification Report (With Hyperparameter Tuning):")
print(classification_report(y_test, y_pred_j48_tuned))
print("J48 Decision Tree - Accuracy Score (With Hyperparameter Tuning):", accuracy_score(y_test, y_pred_j48_tuned))

# Overall metrics after hyperparameter tuning
overall_accuracy_tuned = accuracy_score(y_test, y_pred_j48_tuned)
overall_precision_tuned = precision_score(y_test, y_pred_j48_tuned, average='weighted')
overall_recall_tuned = recall_score(y_test, y_pred_j48_tuned, average='weighted')
overall_f2_tuned = fbeta_score(y_test, y_pred_j48_tuned, beta=2, average='weighted')
print(f"\nJ48 Decision Tree Overall Metrics (With Hyperparameter Tuning): Accuracy: {overall_accuracy_tuned:.4f}, Precision: {overall_precision_tuned:.4f}, Recall: {overall_recall_tuned:.4f}, F2 Score: {overall_f2_tuned:.4f}")

# Plot feature importance after hyperparameter tuning
plt.figure(figsize=(15, 10))
plt.barh(X.columns, best_j48.feature_importances_)
plt.xlabel("Feature Importance")
plt.title("Feature Importance (After Hyperparameter Tuning)")
# plt.show()